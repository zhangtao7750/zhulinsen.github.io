<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[九阴真经总纲]]></title>
    <url>%2F2019%2F02%2F21%2F%E4%B9%9D%E9%98%B4%E7%9C%9F%E7%BB%8F%E6%80%BB%E7%BA%B2%2F</url>
    <content type="text"><![CDATA[天之道，损有余而补不足，是故虚胜实，不足胜有余。 其意博，其理奥，其趣深，天地之象分，阴阳之侯列，变化之由表，死生之昭彰，不谋而遗迹自同，勿约而幽明斯契,稽其言有微，验之事不忒，诚可谓至道之宗，奉生之始矣。假若天机迅发，妙识玄通，成谋虽属乎生知，标格亦资与治训，未尝有行不有送，出不由产者亦。 然刻意研精，探微索隐，或识契真要，则目牛无全，故动则有成，犹有鬼神幽赞，而命世奇杰，时时兼有出焉。 大意： 自然的法则，是通过减损有余的来弥补不足的，因此虚的胜过实的，不够的胜过有多的。 它的意义广博，它的道理奥妙，它的趣味深远，天地的气象分明，阴阳气候依次排列，事物变化有序，生死的规律显现，没有商讨但是留下的规律相同，没有约定但是事务都非常契合，真可以说是所有道理的源头，是养生的开始。 如果天资聪明，就可以明白玄奥的道理，办成事情虽然需要聪明的资质，但是品格风范也来自于先人的教导，还没有人走路不从路上走，出入不由门户的。 如果努力研究其中的精神，探讨其中奥妙的含义，然后能够明白道理的精髓，那就达到熟练高超的地步了，这时行动就能有所成就，像是有鬼神的帮助，那些闻名于世的人，也就经常出现了。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>九阴真经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客添加图片的小技巧]]></title>
    <url>%2F2019%2F02%2F19%2Fhexo%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0%E5%9B%BE%E7%89%87%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[1.在博客目录下安装图片插件，代码如下：1npm install hexo-asset-image --save 2.接下来新建文章，文章所在目录会多出一个同名文件夹，将图片放在文件夹下 3.在文章中输入！：1\[](文件夹名/图片名) 例子： \[](博客示例/pic.jpg) 即可 4.如果是已有文章，可以手动新建一个文件夹，后续步骤同上]]></content>
      <categories>
        <category>hexo博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo new创建文章自动用vscode打开]]></title>
    <url>%2F2019%2F02%2F18%2Fhexo-new%E5%88%9B%E5%BB%BA%E6%96%87%E7%AB%A0%E8%87%AA%E5%8A%A8%E7%94%A8vscode%E6%89%93%E5%BC%80%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;hexo new 命令创建新的文章后，需要重新在文件夹里打开才能编辑，较为麻烦，所以加一个js脚本，实现创建新文件之后自动打开。 &emsp;&emsp;首先在博客目录下创建一个scripts脚本目录，并在scripts新建一个 auto_open.js文件。 &emsp;&emsp;然后打开js文件，输入以下代码： 1234var exec = require('child_process').exec;hexo.on('new', function(data)&#123;exec('start "**C:\Program Files\Microsoft VS Code\Code.exe**" ' + data.path);&#125;);]]></content>
      <categories>
        <category>hexo博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy 爬取百度贴吧全站图片]]></title>
    <url>%2F2019%2F02%2F16%2FScrapy%20%E7%88%AC%E5%8F%96%E7%99%BE%E5%BA%A6%E8%B4%B4%E5%90%A7%E5%85%A8%E7%AB%99%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[用scrapy写一个爬取百度贴吧的爬虫，以壁纸吧为例。 进入壁纸吧，审查元素，找到所有的帖子链接，获取帖子链接 代码：1results = response.css(".threadlist_lz.clearfix a::attr(href)").extract() 设置item 1234567import scrapyclass tiebaItem(scrapy.Item): # define the fields for your item here like: # 图片链接 url = scrapy.Field() # 帖子标题，作为文件夹名存储本帖子的图片 title = scrapy.Field() 进入帖子审查元素，获取帖子内所有图片的链接和帖子名称，存入item 代码 12item[&apos;url&apos;] = response.css(&apos;.BDE_Image::attr(src)&apos;).extract()item[&apos;title&apos;] = response.css(&apos;h3::attr(title)&apos;).extract_first() 因为帖子内的页面不止一页，所以这里选择获取获取当前的页数和总页数，如果当前页不是最后一页，则继续解析下一页，直到最后一页 12345cur_page = response.css('.l_pager.pager_theme_4.pb_list_pager span::text').extract_first()last_page = response.css('.l_posts_num .l_reply_num span::text').extract()[1]if cur_page and last_page and int(cur_page) &lt; int(last_page): next_url = url_page + '?pn=&#123;page&#125;'.format(page=str(int(cur_page)+1)) yield Request(url=next_url, callback=self.image_parse) 通过ImagesPipeline下载所有照片 1234567891011121314151617181920212223242526272829303132333435363738from scrapy.exceptions import DropItemfrom scrapy.pipelines.images import ImagesPipelinefrom scrapy import Requestclass TiebapicPipeline(object): def process_item(self, item, spider): return itemclass ImagesPipeline(ImagesPipeline): ''' 获取item的ulr，生成Request请求，加入队列，等待下载, 同时通过request.meta携带文件夹名 ''' def get_media_requests(self, item, info): for i in item['url']: yield Request(i, meta=&#123;'item': item&#125;) ''' 处理每张照片，返回当下request对象路径和文件名 ''' def file_path(self, request, response=None, info=None): url = request.url file_name = url.split('/')[-1] title = request.meta['item']['title'] path = title+'/'+file_name return path ''' 单个item完成下载处理，通过判断文件路径是否存在，不存在说明下载失败，剔除下载失败的图片 ''' def item_completed(self, results, item, info): image_path = [x['path'] for ok, x in results if ok] if not image_path: raise DropItem('Item contains no images') #item['image_paths'] = image_path return item 修改setting, 启用ImagesPipeline 123ITEM_PIPELINES = &#123; 'tiebapic.pipelines.ImagesPipeline': 300,&#125; 完整代码：https://github.com/ZhuLinsen/Scrapy/tree/master/tiebapic]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy 爬取知乎全站用户信息]]></title>
    <url>%2F2019%2F02%2F15%2FScrapy-%E7%88%AC%E5%8F%96%E7%9F%A5%E4%B9%8E%2F</url>
    <content type="text"><![CDATA[目标：爬取知乎所有用户的个人信息，例如name、id、school等 思路：先确定一个大V，获取他的关注列表和粉丝列表，再获取 粉丝和关注用户 的 关注者和粉丝 的关注和粉丝列表，以此循环调用，可以获得几乎全站用户，再各自获取其个人信息，存储到数据库时加一个去重判断即可。 遇到的问题：在爬取到三千多个用户信息的时候，出现了403，然后ip被封，获取的代理池也不好用，躲不过知乎的反爬虫，所以先这样吧 优化：发起请求时可以再加个判断，如果没有粉丝或者没有关注者的，对应就不再发起请求 参考：静觅大佬的教学视频 代码：https://github.com/ZhuLinsen/Scrapy/tree/master/zhihuuser]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+github的一些使用心得]]></title>
    <url>%2F2019%2F02%2F14%2Fhexo-github%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[选用的是next主题，美则美矣，入门简单但是想要高度定制还是需要花时间去琢磨。 暂时先这样吧！]]></content>
      <categories>
        <category>hexo博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[猪猪,情人节快乐鸭！]]></title>
    <url>%2F2019%2F02%2F13%2Fhello-world%2F</url>
    <content type="text"><![CDATA[猪猪,和你在一起，是一件很愉快的事情！ 虽然会有吵吵闹闹 但是想到猪猪都会觉得很甜 不敢想像没有猪猪的日子。。。 和你在一起，每天都是情人节！]]></content>
      <categories>
        <category>我和猪猪</category>
      </categories>
      <tags>
        <tag>cmq</tag>
        <tag>情人节</tag>
      </tags>
  </entry>
</search>
